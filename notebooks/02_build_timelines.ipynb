{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce6b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78530 posts from 2558 users\n",
      "Labels: ['OCD', 'ADHD', 'aspergers', 'depression', 'ptsd']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "df = pd.read_csv('../data/final_dataset.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} posts from {df['user_id'].nunique()} users\")\n",
    "print(f\"Labels: {df['primary_label'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a12d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: turquoiseturtle01\n",
      "Primary label: OCD\n",
      "Number of posts: 81\n",
      "\n",
      "Timeline (chronological order):\n",
      "--------------------------------------------------\n",
      "Post 1 [2021-07-07]: Through May and June, I started getting obsessive thoughts about things like VHS...\n",
      "Post 2 [2021-07-09]: I have been obsessing over the condition of my DVD collection from May and June....\n",
      "Post 3 [2021-07-10]: I’ve noticed when I get intrusive thoughts, I feel a cold pressure on the left s...\n",
      "Post 4 [2021-07-12]: [removed]...\n",
      "Post 5 [2021-07-15]: [removed]...\n"
     ]
    }
   ],
   "source": [
    "# Pick one user as example\n",
    "example_user = df.groupby('user_id').size().sort_values(ascending=False).index[5]\n",
    "\n",
    "user_posts = df[df['user_id'] == example_user].sort_values('timestamp')\n",
    "\n",
    "print(f\"User: {example_user}\")\n",
    "print(f\"Primary label: {user_posts['primary_label'].iloc[0]}\")\n",
    "print(f\"Number of posts: {len(user_posts)}\")\n",
    "print()\n",
    "print(\"Timeline (chronological order):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (_, row) in enumerate(user_posts.head(5).iterrows()):\n",
    "    text = row['post_text'][:80] if pd.notna(row['post_text']) else \"[empty]\"\n",
    "    print(f\"Post {i+1} [{row['timestamp'][:10]}]: {text}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeline builder function created!\n"
     ]
    }
   ],
   "source": [
    "def build_user_timelines(df, min_posts=5, max_posts=50):\n",
    "   \n",
    "    timelines = {}\n",
    "    labels = {}\n",
    "    \n",
    "    # Group by user\n",
    "    for user_id, user_df in df.groupby('user_id'):\n",
    "        # Sort by timestamp\n",
    "        user_df = user_df.sort_values('timestamp')\n",
    "        \n",
    "        # Skip users with too few posts\n",
    "        if len(user_df) < min_posts:\n",
    "            continue\n",
    "        \n",
    "        # Get post texts \n",
    "        posts = []\n",
    "        for _, row in user_df.iterrows():\n",
    "            title = row['post_title'] if pd.notna(row['post_title']) else \"\"\n",
    "            body = row['post_text'] if pd.notna(row['post_text']) else \"\"\n",
    "            combined = f\"{title} {body}\".strip()\n",
    "            if combined:  # Only add non-empty posts\n",
    "                posts.append(combined)\n",
    "        \n",
    "        # Keep only last max_posts (most recent history)\n",
    "        if len(posts) > max_posts:\n",
    "            posts = posts[-max_posts:]\n",
    "        \n",
    "        # Only keep if we still have enough posts\n",
    "        if len(posts) >= min_posts:\n",
    "            timelines[user_id] = posts\n",
    "            labels[user_id] = user_df['primary_label'].iloc[0]\n",
    "    \n",
    "    return timelines, labels\n",
    "\n",
    "print(\"Timeline builder function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0914d22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built timelines for 2558 users\n",
      "\n",
      "Timeline length statistics:\n",
      "  Min: 5\n",
      "  Max: 50\n",
      "  Mean: 9.6\n",
      "  Median: 7.0\n",
      "\n",
      "Users per label:\n",
      "  OCD: 960\n",
      "  aspergers: 567\n",
      "  ADHD: 456\n",
      "  ptsd: 359\n",
      "  depression: 216\n"
     ]
    }
   ],
   "source": [
    "# Build timelines with minimum 5 posts, maximum 50 posts per user\n",
    "timelines, labels = build_user_timelines(df, min_posts=5, max_posts=50)\n",
    "\n",
    "print(f\"Built timelines for {len(timelines)} users\")\n",
    "print()\n",
    "\n",
    "# Check distribution\n",
    "timeline_lengths = [len(t) for t in timelines.values()]\n",
    "print(\"Timeline length statistics:\")\n",
    "print(f\"  Min: {min(timeline_lengths)}\")\n",
    "print(f\"  Max: {max(timeline_lengths)}\")\n",
    "print(f\"  Mean: {np.mean(timeline_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(timeline_lengths):.1f}\")\n",
    "print()\n",
    "\n",
    "# Labels distribution\n",
    "from collections import Counter\n",
    "label_counts = Counter(labels.values())\n",
    "print(\"Users per label:\")\n",
    "for label, count in sorted(label_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0fb75dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Timeline - User: -PanFrog-\n",
      "Label: OCD\n",
      "Number of posts: 5\n",
      "============================================================\n",
      "\n",
      "[Post 1]\n",
      "Is this ZOCD?? Please someone answer quickly\n",
      "\n",
      "Hello\n",
      "I just had a thought pop up last night about what if I am attracted to animals?\n",
      "It really made me disgusted and I have experienced OCD before, but I recently recovered. I dont feel much anxiety, but I feel uncomfortable and disgusted each time a im...\n",
      "\n",
      "[Post 2]\n",
      "Just a question I was wondering if OCD can make you think It's true and that I should just accept it? Even though I know I don't like those kinds of things, my mind makes it seem like I am the bad person that my mind is telling me. But I don't even agree with the thoughts and I know they're not true...\n",
      "\n",
      "[Post 3]\n",
      "It's me again: ZOCD I have already posted about this but I am unsure so..\n",
      "I have had this theme only for two days now, but I do not feel much anxiety about the thoughts. I felt a lot just now, but i keep thinking It's real and that I'm just using OCD as an excuse. One of my themes from another time ...\n"
     ]
    }
   ],
   "source": [
    "# Look at one complete timeline\n",
    "sample_user = list(timelines.keys())[0]\n",
    "\n",
    "print(f\"Sample Timeline - User: {sample_user}\")\n",
    "print(f\"Label: {labels[sample_user]}\")\n",
    "print(f\"Number of posts: {len(timelines[sample_user])}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, post in enumerate(timelines[sample_user][:3]):  # First 3 posts\n",
    "    print(f\"\\n[Post {i+1}]\")\n",
    "    print(post[:300] + \"...\" if len(post) > 300 else post)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82a04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 2558\n",
      "Train users: 1790 (70.0%)\n",
      "Val users: 384 (15.0%)\n",
      "Test users: 384 (15.0%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get all user IDs and their labels\n",
    "user_ids = list(timelines.keys())\n",
    "user_labels = [labels[uid] for uid in user_ids]\n",
    "\n",
    "print(f\"Total users: {len(user_ids)}\")\n",
    "\n",
    "# First split: 70% train, 30% temp (will become val + test)\n",
    "train_users, temp_users, train_labels, temp_labels = train_test_split(\n",
    "    user_ids, \n",
    "    user_labels,\n",
    "    test_size=0.3,\n",
    "    stratify=user_labels,  # Maintain label proportions\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% of temp = 15% val, 15% test\n",
    "val_users, test_users, val_labels, test_labels = train_test_split(\n",
    "    temp_users,\n",
    "    temp_labels,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train users: {len(train_users)} ({len(train_users)/len(user_ids)*100:.1f}%)\")\n",
    "print(f\"Val users: {len(val_users)} ({len(val_users)/len(user_ids)*100:.1f}%)\")\n",
    "print(f\"Test users: {len(test_users)} ({len(test_users)/len(user_ids)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697bece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution per split:\n",
      "\n",
      "Train:\n",
      "  ADHD: 319\n",
      "  OCD: 672\n",
      "  aspergers: 397\n",
      "  depression: 151\n",
      "  ptsd: 251\n",
      "\n",
      "Val:\n",
      "  ADHD: 68\n",
      "  OCD: 144\n",
      "  aspergers: 85\n",
      "  depression: 33\n",
      "  ptsd: 54\n",
      "\n",
      "Test:\n",
      "  ADHD: 69\n",
      "  OCD: 144\n",
      "  aspergers: 85\n",
      "  depression: 32\n",
      "  ptsd: 54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Label distribution per split:\")\n",
    "print()\n",
    "\n",
    "for split_name, split_labels in [(\"Train\", train_labels), (\"Val\", val_labels), (\"Test\", test_labels)]:\n",
    "    counts = Counter(split_labels)\n",
    "    print(f\"{split_name}:\")\n",
    "    for label in sorted(counts.keys()):\n",
    "        print(f\"  {label}: {counts[label]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d029a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created!\n",
      "\n",
      "X_train: 1790 timelines\n",
      "X_val: 384 timelines\n",
      "X_test: 384 timelines\n",
      "\n",
      "Example - X_train[0]:\n",
      "  Number of posts: 6\n",
      "  Label: ADHD\n",
      "  First post preview: Can I talk about the unexpected and INSANELY WELCOME change Adderall has had on my sex drive?! I hop...\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(user_list, timelines, labels):\n",
    "    \"\"\"Create dataset from list of users\"\"\"\n",
    "    X = [timelines[uid] for uid in user_list]  # List of timelines\n",
    "    y = [labels[uid] for uid in user_list]      # List of labels\n",
    "    return X, y\n",
    "\n",
    "# Create datasets\n",
    "X_train, y_train = create_dataset(train_users, timelines, labels)\n",
    "X_val, y_val = create_dataset(val_users, timelines, labels)\n",
    "X_test, y_test = create_dataset(test_users, timelines, labels)\n",
    "\n",
    "print(\"Datasets created!\")\n",
    "print()\n",
    "print(f\"X_train: {len(X_train)} timelines\")\n",
    "print(f\"X_val: {len(X_val)} timelines\")\n",
    "print(f\"X_test: {len(X_test)} timelines\")\n",
    "print()\n",
    "print(\"Example - X_train[0]:\")\n",
    "print(f\"  Number of posts: {len(X_train[0])}\")\n",
    "print(f\"  Label: {y_train[0]}\")\n",
    "print(f\"  First post preview: {X_train[0][0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18df66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding:\n",
      "  ADHD -> 0\n",
      "  OCD -> 1\n",
      "  aspergers -> 2\n",
      "  depression -> 3\n",
      "  ptsd -> 4\n",
      "\n",
      "y_train_encoded sample: [0 4 0 3 4 1 1 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "# Encode all labels\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(\"Label encoding:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label} -> {i}\")\n",
    "\n",
    "print()\n",
    "print(f\"y_train_encoded sample: {y_train_encoded[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5746f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/processed_data.pkl\n",
      "\n",
      "Contents saved:\n",
      "  X_train: 1790 items\n",
      "  X_val: 384 items\n",
      "  X_test: 384 items\n",
      "  y_train: array of shape (1790,)\n",
      "  y_val: array of shape (384,)\n",
      "  y_test: array of shape (384,)\n",
      "  label_encoder: LabelEncoder\n",
      "  train_users: 1790 items\n",
      "  val_users: 384 items\n",
      "  test_users: 384 items\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save all data\n",
    "data_to_save = {\n",
    "    'X_train': X_train,\n",
    "    'X_val': X_val,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train_encoded,\n",
    "    'y_val': y_val_encoded,\n",
    "    'y_test': y_test_encoded,\n",
    "    'label_encoder': label_encoder,\n",
    "    'train_users': train_users,\n",
    "    'val_users': val_users,\n",
    "    'test_users': test_users\n",
    "}\n",
    "\n",
    "with open('../data/processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "\n",
    "print(\"✓ Saved to data/processed_data.pkl\")\n",
    "print()\n",
    "print(\"Contents saved:\")\n",
    "for key, value in data_to_save.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"  {key}: {len(value)} items\")\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        print(f\"  {key}: array of shape {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3572567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
